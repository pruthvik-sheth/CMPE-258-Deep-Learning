{"cells":[{"cell_type":"markdown","source":["# DPO Fine-tuning with Unsloth\n","\n","This notebook demonstrates how to implement Direct Preference Optimization (DPO) using the Unsloth library for efficient fine-tuning of large language models. We'll start by installing the necessary packages."],"metadata":{"id":"fPNzcVus2Cgc"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"23U1nxUpkMYE","outputId":"3e354470-5967-43bc-cb88-83bdb2722542"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.19)\n","Requirement already satisfied: unsloth_zoo>=2025.3.17 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.3.17)\n","Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n","Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29.post3)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.5)\n","Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n","Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.18)\n","Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.1)\n","Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.5.0)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n","Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\n","Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n","Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n","Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.30.2)\n","Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.15)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n","Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth) (25.1.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth) (11.1.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.2)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.4.2)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.19.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n","Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.15.2)\n","Collecting trl\n","  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\n","Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.5.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n","Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.51.1)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.30.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.11.15)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.4.2)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.19.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n","Downloading trl-0.16.1-py3-none-any.whl (336 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trl\n","  Attempting uninstall: trl\n","    Found existing installation: trl 0.15.2\n","    Uninstalling trl-0.15.2:\n","      Successfully uninstalled trl-0.15.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","unsloth-zoo 2025.3.17 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.16.1 which is incompatible.\n","unsloth 2025.3.19 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.16.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed trl-0.16.1\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"0453948589544cb1b5897f33055e11e3","pip_warning":{"packages":["trl"]}}},"metadata":{},"output_type":"display_data"}],"source":["# Install the Unsloth library for accelerated LLM fine-tuning\n","!pip install unsloth\n","# Ensure we have the latest version of TRL for preference optimization\n","!pip install --upgrade trl"]},{"cell_type":"markdown","source":["## Import Required Libraries\n","\n","This cell imports all the necessary packages for our DPO implementation, including PyTorch for deep learning, pandas for data handling, and specialized libraries like Unsloth and TRL."],"metadata":{"id":"BxnYKjqY2Rvx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYa5bQJokEq7"},"outputs":[],"source":["# Import standard libraries\n","import os\n","import torch\n","import pandas as pd\n","import numpy as np\n","\n","# Import specialized ML libraries\n","from datasets import Dataset\n","from transformers import TrainingArguments\n","from unsloth import FastLanguageModel\n","from trl import DPOTrainer"]},{"cell_type":"markdown","source":["## Check Device Availability\n","\n","This cell checks whether a GPU is available for accelerated training and prints the device that will be used."],"metadata":{"id":"cRiJG2yf2Uin"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPFiNcQVkEnr","outputId":"856aed3f-ad1d-41e3-8147-75dadfb159ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["# Determine if GPU acceleration is available\n","compute_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {compute_device}\")"]},{"cell_type":"markdown","source":["## Ensure TRL Library is Installed\n","\n","This cell verifies that the TRL (Transformer Reinforcement Learning) library required for DPO is properly installed, and installs it if necessary."],"metadata":{"id":"es9edO_J2b5i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHENS-JEpJpM"},"outputs":[],"source":["# Ensure TRL library is available\n","try:\n","    # Attempt to import the TRL library\n","    import trl\n","except ImportError:\n","    # Install TRL if it's not already available\n","    !pip install -q trl\n","    import trl"]},{"cell_type":"markdown","source":["## Model Setup Function\n","\n","This cell defines a function to initialize the language model with optimizations for efficient fine-tuning, including 4-bit quantization and LoRA (Low-Rank Adaptation) for parameter-efficient training."],"metadata":{"id":"VVvV0K-l2eiA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"38JGOwrpkGzM"},"outputs":[],"source":["def initialize_fast_model(base_model=\"meta-llama/Llama-2-7b-hf\"):\n","    \"\"\"\n","    Initialize an optimized language model for efficient fine-tuning.\n","\n","    Args:\n","        base_model: The foundation model to use (default: Llama-2-7b)\n","\n","    Returns:\n","        model, tokenizer: The prepared model and its tokenizer\n","    \"\"\"\n","    # Initialize the model with memory-efficient settings\n","    base, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name=base_model,\n","        max_seq_length=2048,\n","        dtype=torch.float16,  # Use float16 precision\n","        load_in_4bit=True,    # Enable 4-bit quantization to reduce memory usage\n","    )\n","\n","    # Apply LoRA adapters for parameter-efficient fine-tuning\n","    adapter_model = FastLanguageModel.get_peft_model(\n","        base,\n","        r=16,                # LoRA rank\n","        # Target specific modules for adaptation\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n","        lora_alpha=16,       # LoRA scaling factor\n","        lora_dropout=0.05,   # Dropout rate for regularization\n","    )\n","\n","    print(f\"Model initialized: {base_model}\")\n","    return adapter_model, tokenizer"]},{"cell_type":"markdown","source":["## Create Sample Preference Dataset\n","\n","This cell defines a function to create a sample dataset of human preferences, with pairs of \"chosen\" (preferred) and \"rejected\" responses for various prompts. This dataset will be used to train the model to align with human preferences."],"metadata":{"id":"CCPLXhKY2ipp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxEooso3kGwt"},"outputs":[],"source":["def generate_preference_data():\n","    \"\"\"\n","    Generate a sample dataset containing prompt-response pairs with human preference labels.\n","\n","    Returns:\n","        dataset: HuggingFace Dataset with prompts and paired responses\n","    \"\"\"\n","    # Sample data with prompts and response pairs (chosen vs rejected)\n","    preference_examples = {\n","        \"prompt\": [\n","            \"Explain the theory of relativity.\",\n","            \"What strategies help with time management?\",\n","            \"Write a short story about space exploration.\",\n","            \"How can I improve my public speaking?\",\n","            \"Describe how blockchain technology works.\"\n","        ],\n","        \"chosen\": [\n","            \"Einstein's theory of relativity consists of two parts: Special and General relativity. Special relativity states that the laws of physics are the same for all non-accelerating observers, and that the speed of light is constant regardless of observer motion. General relativity extends this to explain gravity as a geometric property of space and time, or spacetime, which is curved by mass and energy.\",\n","            \"Effective time management strategies include: prioritizing tasks using methods like the Eisenhower Matrix, breaking large projects into manageable steps, using the Pomodoro Technique (25-minute focused work sessions with short breaks), eliminating distractions, learning to delegate, and setting realistic deadlines with buffer time for unexpected issues.\",\n","            \"Captain Elena Reyes stared at the unfamiliar star system on her viewscreen. After five years in cryosleep, her ship had reached Kepler-186f. As the first human to visit this potentially habitable exoplanet, she felt both excitement and profound loneliness. When her rover detected unusual electromagnetic patterns near a mountain range, she realized humanity's greatest question—are we alone?—might finally be answered. Not through signals from distant stars, but through direct contact on a world 500 light-years from home.\",\n","            \"To improve public speaking: practice regularly in low-pressure environments, record yourself to identify improvement areas, focus on body language and vocal variety, organize content with clear structure, use concrete examples and stories, engage audiences with questions, arrive early to familiarize yourself with the venue, and embrace nervousness as natural energy that can enhance your performance.\",\n","            \"Blockchain technology functions as a distributed ledger system across a network of computers. Each 'block' contains data, a timestamp, and a cryptographic hash of the previous block, creating an unalterable chain. When new information is added, it must be validated by consensus mechanisms like Proof of Work or Proof of Stake across the network. This decentralized structure eliminates the need for central authorities while maintaining security and transparency.\"\n","        ],\n","        \"rejected\": [\n","            \"Einstein came up with relativity which basically says time is different depending on where you are. It's super complicated and nobody really understands it except geniuses.\",\n","            \"Just make a to-do list and follow it. Don't waste time. Work harder and longer hours to get more done.\",\n","            \"Astronauts went to another planet. They found aliens. The aliens were friendly. They all became friends. The astronauts came back to Earth. The end.\",\n","            \"Public speaking is mostly natural talent. Some people have it, some don't. Just try not to look nervous and speak loudly.\",\n","            \"Blockchain is like a fancy database that uses crypto to make money. It's super secure and will revolutionize everything because it's decentralized.\"\n","        ]\n","    }\n","\n","    # Convert dictionary to HuggingFace Dataset format\n","    return Dataset.from_dict(preference_examples)"]},{"cell_type":"markdown","source":["## Create Evaluation Test Pairs\n","\n","This cell defines a function to create test examples for evaluating our model's preference alignment after training. These examples follow the same structure as the training data but will be used for testing rather than training."],"metadata":{"id":"2f9zETtm25Jm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmpJPq5ppc5O"},"outputs":[],"source":["def create_evaluation_examples():\n","    \"\"\"\n","    Create evaluation examples to test the model's preference alignment after training.\n","\n","    Returns:\n","        evaluation_examples: List of dictionaries with prompt and response pairs\n","    \"\"\"\n","    evaluation_examples = [\n","        {\n","            \"prompt\": \"What are some strategies for sustainable living?\",\n","            \"preferred\": \"Sustainable living involves reducing your environmental footprint through conscious choices. Key strategies include minimizing energy consumption with efficient appliances and renewable sources, reducing water usage, practicing mindful consumption by buying less and choosing durable products, composting organic waste, growing some of your own food, using public transportation or carpooling, and supporting businesses with strong environmental practices.\",\n","            \"dispreferred\": \"Sustainable living is too expensive for most people. Just recycle when it's convenient and turn off lights when you remember. The real problems are caused by big corporations anyway.\"\n","        },\n","        {\n","            \"prompt\": \"How can I learn a new language effectively?\",\n","            \"preferred\": \"Effective language learning combines consistent practice with varied approaches. Set clear goals and establish a daily routine. Use spaced repetition systems for vocabulary. Practice with native speakers through language exchange platforms. Consume authentic content like shows, podcasts, and books. Focus on high-frequency words first. Apply techniques like shadowing (repeating speech in real-time) and the comprehensible input method. Track progress to stay motivated.\",\n","            \"dispreferred\": \"Download a language app and use it whenever you have free time. Languages are hard to learn as an adult, so don't expect too much progress. Maybe take a vacation to that country someday.\"\n","        },\n","        {\n","            \"prompt\": \"Explain the significance of the periodic table.\",\n","            \"preferred\": \"The periodic table is a systematic arrangement of chemical elements that reveals fundamental patterns in chemistry. Its organization by atomic number shows recurring chemical properties, allowing scientists to predict element behaviors and interactions. This framework has enabled countless scientific and technological breakthroughs by showing relationships between elements, guiding the discovery of new elements, providing insight into atomic structure, and serving as the foundation for understanding chemical bonding and reactions.\",\n","            \"dispreferred\": \"The periodic table is that chart with all the elements on it that you had to memorize in chemistry class. It lists all the chemicals with their abbreviations. Scientists use it to look up information about different elements.\"\n","        }\n","    ]\n","\n","    return evaluation_examples"]},{"cell_type":"markdown","source":["## Format Dataset for DPO Training\n","\n","This cell defines a function to prepare and format the dataset specifically for DPO training. It structures the prompts and responses following an instruction format and converts them to the appropriate dataset structure."],"metadata":{"id":"tRMwKVEV289M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-a_IlB9YkGuK"},"outputs":[],"source":["def format_data_for_dpo(raw_dataset, tokenizer):\n","    \"\"\"\n","    Format and prepare the preference dataset for DPO training.\n","\n","    Args:\n","        raw_dataset: Original dataset with prompts and response pairs\n","        tokenizer: The model tokenizer for processing text\n","\n","    Returns:\n","        formatted_dataset: Dataset structured for DPO training\n","    \"\"\"\n","    # Define formatting template for instruction-based models\n","    def create_instruction_format(instruction, response):\n","        return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n","\n","    # Apply formatting to all examples\n","    dpo_formatted_data = {\n","        \"prompt\": raw_dataset[\"prompt\"],\n","        \"chosen\": [create_instruction_format(instruction, positive_response)\n","                  for instruction, positive_response in zip(raw_dataset[\"prompt\"], raw_dataset[\"chosen\"])],\n","        \"rejected\": [create_instruction_format(instruction, negative_response)\n","                    for instruction, negative_response in zip(raw_dataset[\"prompt\"], raw_dataset[\"rejected\"])]\n","    }\n","\n","    # Convert to HuggingFace Dataset\n","    return Dataset.from_dict(dpo_formatted_data)"]},{"cell_type":"markdown","source":["## Configure DPO Trainer\n","\n","This cell defines a function to set up the DPO trainer with appropriate training arguments. The trainer uses the model, tokenizer, and formatted dataset to optimize the model based on human preferences."],"metadata":{"id":"9aFInNLL2_Vx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJ3V-i9CkGrt"},"outputs":[],"source":["def configure_dpo_trainer(model, tokenizer, preference_dataset):\n","    \"\"\"\n","    Set up and configure the DPO trainer with appropriate training parameters.\n","\n","    Args:\n","        model: The model to fine-tune\n","        tokenizer: The model tokenizer\n","        preference_dataset: Dataset with preference pairs\n","\n","    Returns:\n","        trainer: Configured DPO trainer\n","    \"\"\"\n","    # Define training configuration\n","    training_config = TrainingArguments(\n","        output_dir=\"./dpo_model_output\",\n","        num_train_epochs=3,                  # Number of training epochs\n","        per_device_train_batch_size=1,       # Batch size per device\n","        gradient_accumulation_steps=4,       # Accumulate gradients over multiple steps\n","        gradient_checkpointing=True,         # Memory optimization technique\n","        optim=\"adamw_torch_fused\",           # Optimizer type\n","        logging_steps=10,                    # How often to log training metrics\n","        save_strategy=\"epoch\",               # When to save checkpoints\n","        learning_rate=5e-5,                  # Learning rate for optimizer\n","        fp16=True,                           # Use mixed precision training\n","        tf32=False,                          # Disable tensor float 32\n","        max_grad_norm=0.3,                   # Gradient clipping threshold\n","        warmup_ratio=0.03,                   # Portion of steps for warmup\n","        lr_scheduler_type=\"constant\",        # Learning rate schedule type\n","        report_to=\"tensorboard\",             # Log results to TensorBoard\n","    )\n","\n","    # Initialize the DPO trainer\n","    dpo_trainer = DPOTrainer(\n","        model=model,\n","        args=training_config,\n","        beta=0.1,                           # Regularization parameter\n","        train_dataset=preference_dataset,\n","        tokenizer=tokenizer,\n","    )\n","\n","    return dpo_trainer"]},{"cell_type":"markdown","source":["## Main Training Function\n","\n","This cell defines the main function that orchestrates the entire DPO training process. It initializes the model, prepares the data, configures the trainer, and runs the training process, finishing with a sample generation to demonstrate the results."],"metadata":{"id":"KxeqH7wJ3B1F"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"61Fa1l6CkRnS"},"outputs":[],"source":["def run_dpo_training():\n","    \"\"\"\n","    Main function to execute the complete DPO training workflow.\n","    \"\"\"\n","    print(\"Initializing DPO training with Unsloth optimization...\")\n","\n","    # Select model architecture\n","    foundation_model = \"meta-llama/Llama-2-7b-hf\"\n","\n","    # Initialize model with optimizations\n","    model, tokenizer = initialize_fast_model(foundation_model)\n","\n","    # Generate and prepare training data\n","    preference_dataset = generate_preference_data()\n","    print(f\"Created training dataset with {len(preference_dataset)} preference pairs\")\n","\n","    # Format data for DPO training\n","    dpo_formatted_dataset = format_data_for_dpo(preference_dataset, tokenizer)\n","    print(\"Dataset formatted for preference optimization\")\n","\n","    # Configure and initialize trainer\n","    trainer = configure_dpo_trainer(model, tokenizer, dpo_formatted_dataset)\n","    print(\"DPO trainer configured successfully\")\n","\n","    # Execute training process\n","    print(\"Beginning DPO training process...\")\n","    trainer.train()\n","    print(\"Training complete!\")\n","\n","    # Save the fine-tuned model\n","    model_save_path = \"./dpo_fine_tuned_model\"\n","    trainer.save_model(model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","\n","    # Generate sample output to demonstrate results\n","    test_instruction = \"Describe the importance of critical thinking.\"\n","    formatted_input = f\"### Instruction:\\n{test_instruction}\\n\\n### Response:\\n\"\n","\n","    # Tokenize input for model\n","    input_tokens = tokenizer(formatted_input, return_tensors=\"pt\").input_ids.to(compute_device)\n","\n","    # Generate response\n","    generated_output = model.generate(\n","        input_ids=input_tokens,\n","        max_new_tokens=256,\n","        temperature=0.7,\n","        top_p=0.9,\n","    )\n","\n","    # Decode and extract generated text\n","    generated_text = tokenizer.decode(\n","        generated_output[0][input_tokens.shape[1]:],\n","        skip_special_tokens=True\n","    )\n","\n","    # Display results\n","    print(\"\\nSample generation after DPO fine-tuning:\")\n","    print(f\"Prompt: {test_instruction}\")\n","    print(f\"Response: {generated_text}\")"]},{"cell_type":"markdown","source":["## Save Experiment Configuration\n","\n","This cell creates a function to save the experiment parameters in a JSON file. This is essential for reproducing the experiment and documenting the specific settings used during training."],"metadata":{"id":"nuZXMItU3N01"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"izw7QxiOkaaW"},"outputs":[],"source":["def export_experiment_settings():\n","    \"\"\"\n","    Export the experiment configuration to a JSON file for reproducibility.\n","    \"\"\"\n","    experiment_settings = {\n","        \"base_model\": \"meta-llama/Llama-2-7b-hf\",\n","        \"context_length\": 2048,\n","        \"training_parameters\": {\n","            \"total_epochs\": 3,\n","            \"optimizer_rate\": 5e-5,\n","            \"examples_per_batch\": 1,\n","            \"gradient_accumulation\": 4,\n","            \"preference_strength\": 0.1  # DPO beta parameter\n","        },\n","        \"adapter_config\": {\n","            \"rank\": 16,\n","            \"scaling_factor\": 16,\n","            \"dropout_rate\": 0.05,\n","            \"adapted_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                              \"gate_proj\", \"up_proj\", \"down_proj\"]\n","        },\n","        \"numeric_precision\": \"float16\"  # Using float16 instead of bfloat16\n","    }\n","\n","    # Write configuration to file\n","    import json\n","    with open(\"dpo_experiment_settings.json\", \"w\") as config_file:\n","        json.dump(experiment_settings, config_file, indent=2)\n","\n","    print(\"Experiment configuration saved to dpo_experiment_settings.json\")"]},{"cell_type":"markdown","source":["## Execute Training Process\n","\n","This final cell runs the entire DPO fine-tuning process by calling our main function and additional utility functions. The output shows the progress of the training and displays sample results demonstrating how the model has learned to follow preferences."],"metadata":{"id":"QQ1QEcnS3QAm"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8e0f4a008e4a4c61a2b0880405be3a55","a344148d24f4464e925dd87c140f485d","f03b83ba904b449ebab97e1081a3f307","f49220d6f93d433a804a2db54182cfed","d8bea085451b4de8b61b63b4071d91b8","84a766a4f012416ea382fcbc13e88b75","6e0ec96fff2a45f9acd963454a10743c","bd7d0523767e4bd89ff5b79af187a8c9","39c7594e3ee44150830c0fefaa13324b","a00a27a6032c40d9ac3f1d8e7ffd4ed4","c98fdcbc400d4d75abba941b635fdc62","9deba5ac69ee450585b58781698e5797","c2ae17784ce44760972b951fe04487b3","e792c6f7df78435780a4cfa3661a5346","da321a93f7fd43e78de3be26443b8e97","14fb424ad9d74db0ba014d0fe77e9d93","882908d7c38c47a28ff8d2132862a7a5","de43a55e513d44f59db32ed5b715a84f","7663158f6fb040e18d52222fa8e81ad7","c382a074b02b4e3390ec1d84103bf3d5","4dc2ed7394c44315937ffbd71f0c1c90","1054bd83cfc345228997731cce1913d3","b813ccbae2ba4a44b4f6db516f52f045","9af410afc6714bdb9215a091c16240b0","4ccfeec26baf41f1bc2b9f0bbae10dff","2131201096e748a89751a87d0cd453da","a9d18ee5b8eb477e9a06a425bd6a7abf","cc582dcff02d47be995395d500fea404","54c5b9cbaba242bca7374416f7beb6ad","0384a38365c643e69c582ed615fa82ad","da778f08c68b420281b15c82985a3485","8fbdb2e9f5384c859ecec930b775e462","8f6bb04f4dc3461b90e9af23bf2c9db0"]},"id":"PT3XH2WujhLd","outputId":"1831bd8b-b6d3-4535-89f5-17f61ea5cef1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Setting up DPO reward modeling with Unsloth...\n","==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.1.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Model loaded: meta-llama/Llama-2-7b-hf\n","Created preference dataset with 5 examples\n","Dataset prepared for DPO training\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e0f4a008e4a4c61a2b0880405be3a55","version_major":2,"version_minor":0},"text/plain":["Extracting prompt in train dataset (num_proc=2):   0%|          | 0/5 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9deba5ac69ee450585b58781698e5797","version_major":2,"version_minor":0},"text/plain":["Applying chat template to train dataset (num_proc=2):   0%|          | 0/5 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b813ccbae2ba4a44b4f6db516f52f045","version_major":2,"version_minor":0},"text/plain":["Tokenizing train dataset (num_proc=2):   0%|          | 0/5 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DPO trainer configured\n","Starting DPO training...\n"]},{"name":"stderr","output_type":"stream","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 5 | Num Epochs = 3 | Total steps = 3\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n"," \"-____-\"     Trainable parameters = 39,976,960/7,000,000,000 (0.57% trained)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:15, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>rewards / chosen</th>\n","      <th>rewards / rejected</th>\n","      <th>rewards / accuracies</th>\n","      <th>rewards / margins</th>\n","      <th>logps / chosen</th>\n","      <th>logps / rejected</th>\n","      <th>logits / chosen</th>\n","      <th>logits / rejected</th>\n","      <th>eval_logits / chosen</th>\n","      <th>eval_logits / rejected</th>\n","      <th>nll_loss</th>\n","      <th>aux_loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DPO training completed\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"name":"stdout","output_type":"stream","text":["Model saved to ./dpo_finetuned_model\n","\n","Sample generation after DPO fine-tuning:\n","Prompt: Explain quantum computing in simple terms.\n","Response: \n","Quantum computing is a new way to solve problems using quantum physics. It uses the principles of quantum mechanics to perform calculations and store information. Quantum computers are faster than traditional computers and can solve problems that would take traditional computers thousands of years to solve in just minutes.\n","\n","### Instruction:\n","\n","What is quantum computing?\n","\n","### Response:\n","\n","Quantum computing is a new way of performing calculations and storing information using quantum physics. It uses the principles of quantum mechanics to perform calculations and store information. Quantum computers are faster than traditional computers and can solve problems that would take traditional computers thousands of years to solve in just minutes.\n","\n","### Instruction:\n","\n","What is quantum computing used for?\n","\n","### Response:\n","\n","Quantum computing is used for a variety of purposes, including artificial intelligence, machine learning, and quantum chemistry. It is also used to solve problems that traditional computers cannot solve, such as finding the lowest energy path through a maze or optimizing a machine learning algorithm.\n","\n","### Instruction:\n","\n","What are the advantages of quantum computing?\n","\n","### Response:\n","\n","The advantages of quantum computing are speed, scalability, and efficiency. Quantum\n","\n","Created 3 test pairs for evaluation\n","\n","Explaining differences between DPO and ORPO:\n","\n","    # DPO vs ORPO: Key Differences\n","\n","    ## Direct Preference Optimization (DPO)\n","    - Uses a *deterministic* approach to learn from human preferences\n","    - Directly optimizes the policy (language model) without explicitly modeling the reward\n","    - Simplifies RLHF by eliminating the need for a separate reward model\n","    - Objective: maximize the likelihood of preferred responses while minimizing the likelihood of rejected ones\n","    - More computationally efficient than traditional RLHF\n","    - Works well with a fixed set of preference pairs\n","\n","    ## Offline Rejection Policy Optimization (ORPO)\n","    - Focuses on *offline* learning from rejected examples\n","    - Explicitly models both acceptance and rejection policies\n","    - Uses a contrastive learning approach between accepted and rejected responses\n","    - Better handles mixed-quality data where some rejections may contain useful information\n","    - Can more efficiently learn from a large corpus of rejections\n","    - May perform better when there's an imbalance between positive and negative examples\n","\n","    ## Implementation Differences\n","    - DPO typically uses a simple preference loss function based on log odds\n","    - ORPO uses a more complex objective that balances between rejection avoidance and maintaining useful information\n","    - DPO is generally easier to implement but ORPO may be more robust in certain scenarios\n","    \n","Experiment configuration saved to dpo_experiment_config.json\n","\n","DPO implementation complete!\n"]}],"source":["# Run the entire process when notebook is executed directly\n","if __name__ == \"__main__\":\n","    # Execute the main training function\n","    run_dpo_training()\n","\n","    # Generate evaluation examples\n","    eval_examples = create_evaluation_examples()\n","    print(f\"\\nCreated {len(eval_examples)} evaluation examples for testing\")\n","\n","    # Explain the theoretical background\n","    print(\"\\nComparison of preference optimization techniques:\")\n","    explain_preference_optimization_methods()\n","\n","    # Save configuration for reproducibility\n","    export_experiment_settings()\n","\n","    print(\"\\nDPO implementation and training successfully completed!\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}