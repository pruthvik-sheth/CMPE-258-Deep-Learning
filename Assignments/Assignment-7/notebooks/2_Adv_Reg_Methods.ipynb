{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNPpKmIqFmB+ON8oG2DMCpQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Advanced Regularization Methods\n","\n","This notebook explores more sophisticated regularization techniques in TensorFlow:\n","\n","1. Weight initialization strategies\n","2. Batch normalization\n","3. Custom dropout and regularization implementations\n","4. Callbacks and TensorBoard integration\n","\n","We'll demonstrate how these techniques affect model performance and provide code for implementing them in your own projects."],"metadata":{"id":"wGESY20fwMlK"}},{"cell_type":"markdown","source":["## Data Preparation for Advanced Regularization\n","\n","In this notebook, we'll explore advanced regularization techniques. We start by loading the Fashion MNIST dataset and preparing it for our experiments.\n","\n","Key aspects of the data preparation:\n","1. **Data Normalization**: Pixel values are scaled to the range [0, 1] by dividing by 255.\n","2. **Validation Split**: We separate a portion of the training data to create a validation set for monitoring model performance.\n","3. **Dataset Reduction**: To speed up execution, we use a smaller subset of the training data (10,000 examples).\n","\n","This setup allows us to quickly test different regularization techniques while still providing meaningful comparisons."],"metadata":{"id":"4EtlN6R7wmF0"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trh1IqmowGv2","executionInfo":{"status":"ok","timestamp":1746151963430,"user_tz":420,"elapsed":4706,"user":{"displayName":"Pruthvik Sheth","userId":"03427505168681154907"}},"outputId":"be8661b7-b986-4395-ce6e-e790dd4049da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Training set shape: (10000, 28, 28)\n","Validation set shape: (5000, 28, 28)\n","Test set shape: (10000, 28, 28)\n"]}],"source":["# Import necessary libraries\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.initializers import RandomNormal, GlorotNormal, GlorotUniform, HeNormal, HeUniform\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import datetime\n","\n","# Load and preprocess the Fashion MNIST dataset\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n","\n","# Split the training data to create a validation set\n","val_split = 5000\n","x_val, y_val = x_train[:val_split], y_train[:val_split]\n","x_train, y_train = x_train[val_split:], y_train[val_split:]\n","\n","# Reduce dataset size for faster execution\n","train_size = 10000\n","x_train, y_train = x_train[:train_size], y_train[:train_size]\n","\n","print(f\"Training set shape: {x_train.shape}\")\n","print(f\"Validation set shape: {x_val.shape}\")\n","print(f\"Test set shape: {x_test.shape}\")"]},{"cell_type":"markdown","source":["## Weight Initialization Strategies\n","\n","Weight initialization is crucial for neural network performance. Poor initialization can lead to vanishing or exploding gradients, slowing down or preventing learning.\n","\n","This code compares five common initialization strategies:\n","\n","1. **RandomNormal**: Initializes weights from a normal distribution with small variance. Not optimal for deep networks.\n","\n","2. **GlorotNormal/GlorotUniform (Xavier)**: Designed to maintain the scale of gradients throughout the network. Works well with symmetric activation functions like tanh.\n","\n","3. **HeNormal/HeUniform**: Variants of Xavier initialization that account for the non-linearity of ReLU activations. Generally preferred for ReLU networks.\n","\n","Each model is trained for 5 epochs with identical architecture except for the weight initialization strategy. The training histories are stored for comparison to determine which strategy works best for this dataset.\n","\n","When to use each initializer:\n","- For ReLU activation: He initializers\n","- For tanh/sigmoid activation: Glorot initializers\n","- For very deep networks: Specialized initializers may be needed"],"metadata":{"id":"9orSelJVwu_R"}},{"cell_type":"code","source":["# Create models with different weight initialization strategies\n","def create_model_with_initializer(initializer_name, initializer):\n","    model = Sequential([\n","        Flatten(input_shape=(28, 28)),\n","        Dense(128, activation='relu', kernel_initializer=initializer),\n","        Dense(64, activation='relu', kernel_initializer=initializer),\n","        Dense(10, activation='softmax', kernel_initializer=initializer)\n","    ])\n","\n","    model.compile(\n","        optimizer='adam',\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","    return model\n","\n","# Define different initializers to compare\n","initializers = {\n","    'RandomNormal': RandomNormal(mean=0.0, stddev=0.01),\n","    'GlorotNormal': GlorotNormal(),  # Suitable for tanh activation\n","    'GlorotUniform': GlorotUniform(),  # Xavier uniform, good for tanh\n","    'HeNormal': HeNormal(),  # Better for ReLU activation\n","    'HeUniform': HeUniform()  # Better for ReLU activation\n","}\n","\n","# Train models with different initializers\n","histories = {}\n","EPOCHS = 5\n","BATCH_SIZE = 128\n","\n","for name, initializer in initializers.items():\n","    print(f\"Training model with {name} initializer...\")\n","    model = create_model_with_initializer(name, initializer)\n","    history = model.fit(\n","        x_train, y_train,\n","        epochs=EPOCHS,\n","        batch_size=BATCH_SIZE,\n","        validation_data=(x_val, y_val),\n","        verbose=1\n","    )\n","    histories[name] = history"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHFTRdGmwsPR","executionInfo":{"status":"ok","timestamp":1746152048826,"user_tz":420,"elapsed":85387,"user":{"displayName":"Pruthvik Sheth","userId":"03427505168681154907"}},"outputId":"24460c89-043f-4541-8082-eca36184ccd4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Training model with RandomNormal initializer...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.2964 - loss: 1.8895 - val_accuracy: 0.6128 - val_loss: 0.9352\n","Epoch 2/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6355 - loss: 0.8879 - val_accuracy: 0.7168 - val_loss: 0.7782\n","Epoch 3/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7149 - loss: 0.7452 - val_accuracy: 0.7276 - val_loss: 0.7082\n","Epoch 4/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7462 - loss: 0.6576 - val_accuracy: 0.7764 - val_loss: 0.6228\n","Epoch 5/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7723 - loss: 0.6126 - val_accuracy: 0.7984 - val_loss: 0.5885\n","Training model with GlorotNormal initializer...\n","Epoch 1/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5963 - loss: 1.1732 - val_accuracy: 0.7984 - val_loss: 0.5989\n","Epoch 2/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8084 - loss: 0.5523 - val_accuracy: 0.8304 - val_loss: 0.5081\n","Epoch 3/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8261 - loss: 0.5041 - val_accuracy: 0.8286 - val_loss: 0.4845\n","Epoch 4/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8488 - loss: 0.4212 - val_accuracy: 0.8362 - val_loss: 0.4650\n","Epoch 5/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8548 - loss: 0.3930 - val_accuracy: 0.8418 - val_loss: 0.4456\n","Training model with GlorotUniform initializer...\n","Epoch 1/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.5712 - loss: 1.2869 - val_accuracy: 0.7838 - val_loss: 0.6100\n","Epoch 2/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8020 - loss: 0.5518 - val_accuracy: 0.8258 - val_loss: 0.5108\n","Epoch 3/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8353 - loss: 0.4718 - val_accuracy: 0.8376 - val_loss: 0.4646\n","Epoch 4/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8428 - loss: 0.4302 - val_accuracy: 0.8506 - val_loss: 0.4279\n","Epoch 5/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8643 - loss: 0.3908 - val_accuracy: 0.8334 - val_loss: 0.4643\n","Training model with HeNormal initializer...\n","Epoch 1/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.6146 - loss: 1.1785 - val_accuracy: 0.8192 - val_loss: 0.5452\n","Epoch 2/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8102 - loss: 0.5398 - val_accuracy: 0.8376 - val_loss: 0.4759\n","Epoch 3/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8320 - loss: 0.4705 - val_accuracy: 0.8252 - val_loss: 0.4951\n","Epoch 4/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8395 - loss: 0.4554 - val_accuracy: 0.8272 - val_loss: 0.4898\n","Epoch 5/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8535 - loss: 0.4102 - val_accuracy: 0.8392 - val_loss: 0.4491\n","Training model with HeUniform initializer...\n","Epoch 1/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.5523 - loss: 1.2935 - val_accuracy: 0.7952 - val_loss: 0.5849\n","Epoch 2/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8071 - loss: 0.5626 - val_accuracy: 0.8144 - val_loss: 0.5266\n","Epoch 3/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8193 - loss: 0.5052 - val_accuracy: 0.8410 - val_loss: 0.4634\n","Epoch 4/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8393 - loss: 0.4368 - val_accuracy: 0.8546 - val_loss: 0.4272\n","Epoch 5/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8594 - loss: 0.3878 - val_accuracy: 0.8428 - val_loss: 0.4390\n"]}]},{"cell_type":"markdown","source":["## Batch Normalization\n","\n","Batch normalization stabilizes training by normalizing layer inputs for each mini-batch. This implementation places BatchNorm layers between Dense layers and activations, allowing the network to learn the optimal activation distribution.\n","\n","Key benefits:\n","- Speeds up training by reducing internal covariate shift\n","- Acts as a regularizer, often reducing the need for dropout\n","- Allows higher learning rates\n","- Makes the model less sensitive to weight initialization"],"metadata":{"id":"V_vSMn1MxKUL"}},{"cell_type":"code","source":["# Create a model with Batch Normalization\n","def create_batch_norm_model():\n","    model = Sequential([\n","        Flatten(input_shape=(28, 28)),\n","        Dense(128),  # No activation here since BatchNorm will be applied before activation\n","        BatchNormalization(),\n","        tf.keras.layers.Activation('relu'),\n","        Dense(64),\n","        BatchNormalization(),\n","        tf.keras.layers.Activation('relu'),\n","        Dense(10, activation='softmax')\n","    ])\n","\n","    model.compile(\n","        optimizer='adam',\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","    return model\n","\n","# Train the batch normalization model\n","batch_norm_model = create_batch_norm_model()\n","batch_norm_history = batch_norm_model.fit(\n","    x_train, y_train,\n","    epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    validation_data=(x_val, y_val),\n","    verbose=1\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xuj9xXOJwseo","executionInfo":{"status":"ok","timestamp":1746152048920,"user_tz":420,"elapsed":91,"user":{"displayName":"Pruthvik Sheth","userId":"03427505168681154907"}},"outputId":"45558869-11f2-44af-9805-142e97902793"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.6447 - loss: 1.1119 - val_accuracy: 0.8072 - val_loss: 0.8414\n","Epoch 2/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8376 - loss: 0.4830 - val_accuracy: 0.8418 - val_loss: 0.5531\n","Epoch 3/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8650 - loss: 0.4012 - val_accuracy: 0.8398 - val_loss: 0.4721\n","Epoch 4/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8742 - loss: 0.3582 - val_accuracy: 0.8324 - val_loss: 0.4740\n","Epoch 5/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8973 - loss: 0.3007 - val_accuracy: 0.8522 - val_loss: 0.4259\n"]}]},{"cell_type":"markdown","source":["## Custom Dropout Implementation\n","\n","This code implements a custom dropout layer by subclassing `tf.keras.layers.Layer`. The layer applies random dropout to input units during training but passes through inputs unchanged during inference.\n","\n","Our implementation mirrors standard dropout functionality but allows for future customization of dropout behavior for specialized applications."],"metadata":{"id":"Xe1ocrzExPMh"}},{"cell_type":"code","source":["# Custom Dropout Layer\n","class CustomDropout(tf.keras.layers.Layer):\n","    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n","        super(CustomDropout, self).__init__(**kwargs)\n","        self.rate = rate\n","        self.noise_shape = noise_shape\n","        self.seed = seed\n","\n","    def call(self, inputs, training=None):\n","        if training:\n","            return tf.nn.dropout(\n","                inputs,\n","                rate=self.rate,\n","                noise_shape=self.noise_shape,\n","                seed=self.seed\n","            )\n","        return inputs\n","\n","    def get_config(self):\n","        config = super(CustomDropout, self).get_config()\n","        config.update({\n","            'rate': self.rate,\n","            'noise_shape': self.noise_shape,\n","            'seed': self.seed,\n","        })\n","        return config\n","\n","# Create a model with custom dropout\n","def create_custom_dropout_model(dropout_rate=0.3):\n","    model = Sequential([\n","        Flatten(input_shape=(28, 28)),\n","        Dense(128, activation='relu'),\n","        CustomDropout(dropout_rate),\n","        Dense(64, activation='relu'),\n","        CustomDropout(dropout_rate),\n","        Dense(10, activation='softmax')\n","    ])\n","\n","    model.compile(\n","        optimizer='adam',\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","    return model\n","\n","# Train the custom dropout model\n","custom_dropout_model = create_custom_dropout_model()\n","custom_dropout_history = custom_dropout_model.fit(\n","    x_train, y_train,\n","    epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    validation_data=(x_val, y_val),\n","    verbose=1\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sq8Gcg3rxA1r","executionInfo":{"status":"ok","timestamp":1746152048950,"user_tz":420,"elapsed":23,"user":{"displayName":"Pruthvik Sheth","userId":"03427505168681154907"}},"outputId":"4a8f89e7-e40b-4d2f-f4a0-7dd0fdcc3ebb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.4254 - loss: 1.6172 - val_accuracy: 0.7580 - val_loss: 0.6932\n","Epoch 2/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6947 - loss: 0.8184 - val_accuracy: 0.8114 - val_loss: 0.5544\n","Epoch 3/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7564 - loss: 0.6725 - val_accuracy: 0.8356 - val_loss: 0.4905\n","Epoch 4/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7887 - loss: 0.5794 - val_accuracy: 0.8340 - val_loss: 0.4699\n","Epoch 5/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8002 - loss: 0.5509 - val_accuracy: 0.8416 - val_loss: 0.4514\n"]}]},{"cell_type":"markdown","source":["## Custom Regularizer and TensorBoard Setup\n","\n","This code implements a custom L1L2 regularizer that combines both L1 and L2 penalties in a single regularizer. We also set up TensorBoard for visualization of model training metrics and architecture."],"metadata":{"id":"tQFCnS_dxqjj"}},{"cell_type":"code","source":["# Custom L1L2 Regularizer\n","class CustomL1L2Regularizer(tf.keras.regularizers.Regularizer):\n","    def __init__(self, l1=0.0, l2=0.0):\n","        self.l1 = l1\n","        self.l2 = l2\n","\n","    def __call__(self, weights):\n","        l1_loss = tf.reduce_sum(tf.abs(weights)) * self.l1\n","        l2_loss = tf.reduce_sum(tf.square(weights)) * self.l2\n","        return l1_loss + l2_loss\n","\n","    def get_config(self):\n","        return {'l1': float(self.l1), 'l2': float(self.l2)}\n","\n","# Set up TensorBoard\n","log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(\n","    log_dir=log_dir,\n","    histogram_freq=1,\n","    write_graph=True,\n","    write_images=True,\n","    update_freq='epoch'\n",")\n","\n","# Create a model with custom regularization\n","def create_custom_reg_model(l1=0.001, l2=0.001):\n","    model = Sequential([\n","        Flatten(input_shape=(28, 28)),\n","        Dense(128, activation='relu', kernel_regularizer=CustomL1L2Regularizer(l1=l1, l2=l2)),\n","        Dense(64, activation='relu', kernel_regularizer=CustomL1L2Regularizer(l1=l1, l2=l2)),\n","        Dense(10, activation='softmax')\n","    ])\n","\n","    model.compile(\n","        optimizer='adam',\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","    return model"],"metadata":{"id":"mWjMZjr6xN56","executionInfo":{"status":"ok","timestamp":1746152107732,"user_tz":420,"elapsed":47,"user":{"displayName":"Pruthvik Sheth","userId":"03427505168681154907"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Training with Callbacks and Evaluation\n","\n","This code demonstrates the use of various callbacks in Keras:\n","\n","1. **LearningRateScheduler**: Dynamically adjusts learning rate during training\n","2. **ModelCheckpoint**: Saves the best model based on validation accuracy\n","3. **TensorBoard**: Records metrics for visualization\n","\n","We then compare all the regularization techniques by plotting validation accuracy and printing final performance metrics."],"metadata":{"id":"QyDaCe3Exv5u"}},{"cell_type":"code","source":["# Create LearningRateScheduler callback - fixing the function to return float\n","def lr_scheduler(epoch, lr):\n","    if epoch < 3:\n","        return float(lr)  # Explicitly convert to float\n","    else:\n","        return float(lr * 0.9)  # Use direct multiplication instead of exp\n","\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","\n","# Create ModelCheckpoint callback\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    'best_model.h5',\n","    monitor='val_accuracy',\n","    save_best_only=True,\n","    mode='max',\n","    verbose=1\n",")\n","\n","# Train with custom regularization and callbacks\n","custom_reg_model = create_custom_reg_model()\n","custom_reg_history = custom_reg_model.fit(\n","    x_train, y_train,\n","    epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    validation_data=(x_val, y_val),\n","    callbacks=[tensorboard_callback, lr_callback, checkpoint_callback],\n","    verbose=1\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceNbsfo_xq4e","executionInfo":{"status":"ok","timestamp":1746152285197,"user_tz":420,"elapsed":6616,"user":{"displayName":"Pruthvik Sheth","userId":"03427505168681154907"}},"outputId":"589fe38b-a0d1-42d1-9dba-63d1ec8222b5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5292 - loss: 5.6047\n","Epoch 1: val_accuracy improved from -inf to 0.76980, saving model to best_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.5309 - loss: 5.5899 - val_accuracy: 0.7698 - val_loss: 2.7696 - learning_rate: 0.0010\n","Epoch 2/5\n","\u001b[1m70/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7759 - loss: 2.4429\n","Epoch 2: val_accuracy improved from 0.76980 to 0.79440, saving model to best_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7765 - loss: 2.4099 - val_accuracy: 0.7944 - val_loss: 1.7532 - learning_rate: 0.0010\n","Epoch 3/5\n","\u001b[1m71/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7893 - loss: 1.6601\n","Epoch 3: val_accuracy improved from 0.79440 to 0.79560, saving model to best_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7897 - loss: 1.6496 - val_accuracy: 0.7956 - val_loss: 1.4059 - learning_rate: 0.0010\n","Epoch 4/5\n","\u001b[1m69/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8083 - loss: 1.3707\n","Epoch 4: val_accuracy improved from 0.79560 to 0.80940, saving model to best_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8073 - loss: 1.3661 - val_accuracy: 0.8094 - val_loss: 1.2549 - learning_rate: 9.0000e-04\n","Epoch 5/5\n","\u001b[1m71/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8089 - loss: 1.2354\n","Epoch 5: val_accuracy did not improve from 0.80940\n","\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8085 - loss: 1.2334 - val_accuracy: 0.7958 - val_loss: 1.1914 - learning_rate: 8.1000e-04\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iW4s4cI2xt5Z"},"execution_count":null,"outputs":[]}]}